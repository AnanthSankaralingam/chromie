"use client"

import { useState, useRef, useEffect } from "react"
import { Button } from "@/components/ui/button"
import { Textarea } from "@/components/ui/forms-and-input/textarea"
import { Send, Loader2, ChevronDown, ChevronRight } from "lucide-react"
import { REQUEST_TYPES } from "@/lib/prompts/request-types"
import ModalUrlPrompt from "@/components/ui/modals/modal-url-prompt"
import ChatMessage from "@/components/ui/chat/chat-message"

export default function StreamingChat({ 
  projectId, 
  projectName, 
  autoGeneratePrompt, 
  onAutoGenerateComplete, 
  onCodeGenerated, 
  onGenerationStart, 
  onGenerationEnd, 
  isProjectReady,
  modelOverride
}) {
  const [inputMessage, setInputMessage] = useState("")
  const [messages, setMessages] = useState([
    {
      role: "assistant",
      content: "hi! i'm **chromie**, your chrome extension assistant. tell me what you'd like in your extension.",
    },
  ])
  const [isGenerating, setIsGenerating] = useState(false)
  const [hasGeneratedCode, setHasGeneratedCode] = useState(false)
  const [urlPromptData, setUrlPromptData] = useState(null)
  const [showUrlPrompt, setShowUrlPrompt] = useState(false)
  const messagesEndRef = useRef(null)
  const autoGeneratedRef = useRef(false)
  const currentRequestRef = useRef(null)
  const currentAssistantMessageRef = useRef(null) // Track current streaming message
  const thinkingBufferRef = useRef("") // Buffer for accumulating thinking content
  const thinkingMessageIndexRef = useRef(null) // Track the thinking message index
  const explanationBufferRef = useRef("") // Buffer for accumulating final explanation tokens
  const sentGeneratingMessageRef = useRef(false) // Ensure generating message shows only once
  const previousResponseIdRef = useRef(null)
  const [conversationTokenTotal, setConversationTokenTotal] = useState(0)
  const [typingCancelSignal, setTypingCancelSignal] = useState(0)
  // Gemini thinking panel state (per request)
  const [isModelThinkingOpen, setIsModelThinkingOpen] = useState(false)
  const [modelThinkingFull, setModelThinkingFull] = useState("")
  const [modelThinkingDisplay, setModelThinkingDisplay] = useState("")
  const thinkingTimerRef = useRef(null)
  const thinkingTokensRef = useRef([])
  const thinkingIdxRef = useRef(0)
  const thinkingChunkCountRef = useRef(0)

  // Reset local-only conversation state on project change (navigation/refresh)
  useEffect(() => {
    previousResponseIdRef.current = null
    setConversationTokenTotal(0)
  }, [projectId])

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" })
  }

  // Debug logging
  useEffect(() => {
    // Component mounted
  }, [projectId, isProjectReady])

  // No cleanup needed since we removed buffer timeout

  // Only scroll to bottom when assistant messages are added or streaming content changes
  useEffect(() => {
    if (messages.length > 0) {
      const lastMessage = messages[messages.length - 1]
      if (lastMessage.role === 'assistant') {
        scrollToBottom()
      }
    }
  }, [messages])
  
  // Scroll when new messages are added
  useEffect(() => {
    scrollToBottom()
  }, [messages])

  // Debug: Track modelThinkingFull changes
  useEffect(() => {
    console.log('[streaming-chat] modelThinkingFull changed:', {
      length: modelThinkingFull?.length || 0,
      content: modelThinkingFull?.substring(0, 100) + '...'
    })
  }, [modelThinkingFull])

  // Adaptive typing for model thinking panel (slightly larger batches)
  useEffect(() => {
    if (!modelThinkingFull) return
    // Build token list on new text arrival
    const tokenize = (text) => {
      const sentences = text.split(/(?<=[.!?])\s+/)
      const result = []
      for (const s of sentences) {
        const parts = s.split(/(\s+)/)
        for (const p of parts) { if (p) result.push(p) }
        result.push(" ")
      }
      return result
    }
    thinkingTokensRef.current = tokenize(modelThinkingFull)
    const total = thinkingTokensRef.current.length
    const step = () => {
      const base = total < 40 ? 2 : total < 120 ? 3 : total < 250 ? 5 : 8
      const next = Math.min(thinkingIdxRef.current + base, total)
      const slice = thinkingTokensRef.current.slice(0, next).join("")
      thinkingIdxRef.current = next
      setModelThinkingDisplay(slice)
      if (next < total) {
        const delay = total < 40 ? 28 : total < 120 ? 18 : total < 250 ? 12 : 9
        thinkingTimerRef.current = setTimeout(step, delay)
      }
    }
    if (thinkingTimerRef.current) clearTimeout(thinkingTimerRef.current)
    thinkingTimerRef.current = setTimeout(step, 8)
    return () => { if (thinkingTimerRef.current) clearTimeout(thinkingTimerRef.current) }
  }, [modelThinkingFull])

  useEffect(() => {
    if (autoGeneratePrompt && isProjectReady && !hasGeneratedCode && !isGenerating && !autoGeneratedRef.current) {
      autoGeneratedRef.current = true // Prevent duplicate auto-generation
      setInputMessage(autoGeneratePrompt)
      
      // Add the user message immediately
      const userMessage = {
        role: "user",
        content: autoGeneratePrompt,
      }
      setMessages((prev) => [...prev, userMessage])
      
      // Start the generation process
      startGeneration(autoGeneratePrompt, true) // true indicates this is auto-generation
    }
  }, [autoGeneratePrompt, isProjectReady, hasGeneratedCode, isGenerating])

  const startGeneration = async (prompt, isAutoGeneration = false) => {
    if (isGenerating) return
    
    setIsGenerating(true)
    
    // Reset current assistant message tracking
    currentAssistantMessageRef.current = null
    thinkingBufferRef.current = ""
    thinkingMessageIndexRef.current = null
    explanationBufferRef.current = ""
    sentGeneratingMessageRef.current = false
    // Reset model thinking panel per request
    setIsModelThinkingOpen(false)
    setModelThinkingFull("")
    setModelThinkingDisplay("")
    thinkingTokensRef.current = []
    thinkingIdxRef.current = 0
    thinkingChunkCountRef.current = 0
    if (thinkingTimerRef.current) { clearTimeout(thinkingTimerRef.current); thinkingTimerRef.current = null }

    if (onGenerationStart) {
      onGenerationStart()
    }

    try {
      // Force refresh hasGeneratedCode from Supabase before determining request type
      let currentHasGeneratedCode = hasGeneratedCode
      if (projectId) {
        try {
          const response = await fetch(`/api/projects/${projectId}/has-generated-code`)
          if (response.ok) {
            const data = await response.json()
            currentHasGeneratedCode = data.hasGeneratedCode
            if (data.hasGeneratedCode !== hasGeneratedCode) {
              setHasGeneratedCode(data.hasGeneratedCode)
            }
          }
        } catch (error) {
          console.error('Error refreshing hasGeneratedCode:', error)
        }
      }
      
      const requestType = currentHasGeneratedCode ? REQUEST_TYPES.ADD_TO_EXISTING : REQUEST_TYPES.NEW_EXTENSION
      const hasPrev = Boolean(previousResponseIdRef.current)
      const pathUsed = requestType === REQUEST_TYPES.ADD_TO_EXISTING && hasPrev ? 'responses_api' : 'manual_file_context'
      console.log('[client/streaming-chat] generation params', { 
        requestType, 
        has_previousResponseId: hasPrev, 
        previousResponseId: previousResponseIdRef.current,
        pathUsed, 
        modelOverride: modelOverride || null 
      })

      // Start streaming response
      const response = await fetch("/api/generate/stream", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          prompt: prompt,
          projectId,
          requestType: requestType,
          previousResponseId: previousResponseIdRef.current,
          conversationTokenTotal,
          modelOverride
        }),
      })

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }

      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let buffer = ""

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || ""

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            try {
              const data = JSON.parse(line.slice(6))

              // Helper function to add new assistant message
              const addNewAssistantMessage = (content) => {
                const newMessage = {
                  role: "assistant",
                  content: content
                }
                setMessages(prev => [...prev, newMessage])
              }

              switch (data.type) {
                case "thinking_chunk":
                case "thinking":
                  // Append Gemini thinking text to model thinking panel
                  console.log('[streaming-chat] received thinking event:', data.type, data)
                  console.log('[streaming-chat] thinking content length:', data.content?.length || 0)
                  console.log('[streaming-chat] current modelThinkingFull length:', modelThinkingFull.length)
                  if (typeof data.content === 'string' && data.content.length > 0) {
                    thinkingChunkCountRef.current += 1
                    setModelThinkingFull(prev => {
                      const newContent = prev + data.content
                      console.log('[streaming-chat] updated modelThinkingFull length:', newContent.length, 'chunk count:', thinkingChunkCountRef.current)
                      return newContent
                    })
                    // Keep thinking panel collapsed by default; user can expand
                    console.log('[streaming-chat] thinking chunk received, modelThinkingFull should be visible')
                  }
                  break
                case "start":
                  console.log("ðŸš€ Stream started")
                  addNewAssistantMessage("Starting to analyze your request...")
                  break
                case "token_usage":
                  if (typeof data.total === 'number') {
                    setConversationTokenTotal(data.total)
                    console.log('[client/streaming-chat] token_usage received', { total: data.total })
                  }
                  break
                case "context_window":
                  addNewAssistantMessage('Context limit reached. Please start a new conversation.')
                  if (typeof data.total === 'number') {
                    console.log('[client/streaming-chat] context-window stream handled', { total: data.total })
                    setConversationTokenTotal(data.total)
                  }
                  // Do not continue processing further
                  break
                case "response_id":
                  console.log('[client/streaming-chat] received response_id', { id: data.id, tokensUsedThisRequest: data.tokensUsedThisRequest })
                  previousResponseIdRef.current = data.id
                  if (typeof data.tokensUsedThisRequest === 'number') {
                    const nextTotal = (conversationTokenTotal || 0) + data.tokensUsedThisRequest
                    console.log('[client/streaming-chat] update totals', { id: data.id, tokensUsedThisRequest: data.tokensUsedThisRequest, nextTotal })
                    setConversationTokenTotal(nextTotal)
                  }
                  break

                // Ignore intermediate status noise
                case "analyzing":
                case "analysis_complete":
                case "fetching_apis":
                case "apis_ready":
                case "scraping":
                case "scraping_complete":
                case "scraping_skipped":
                case "context_ready":
                case "generation_starting":
                  break

                case "phase":
                  // Do not render phase updates to keep UI clean
                  break

                case "explanation":
                  // Buffer explanation tokens; emit once on done
                  if (data.content) {
                    explanationBufferRef.current += data.content
                  }
                  break

                case "generating_code":
                  break

                case "code":
                  // If backend supplies file path info, auto-select in editor via global event
                  try {
                    const filePath = data.file_path || data.path || data.file || null
                    if (filePath && typeof window !== 'undefined') {
                      const evt = new CustomEvent('editor:selectFile', { detail: { file_path: String(filePath) } })
                      window.dispatchEvent(evt)
                    }
                  } catch (_) {}
                  break
                case "files_saved":
                case "generation_complete":
                  // On save or completion, try to focus manifest.json after a short delay
                  try {
                    if (typeof window !== 'undefined') {
                      setTimeout(() => {
                        const evt = new CustomEvent('editor:focusManifest')
                        window.dispatchEvent(evt)
                      }, 200)
                    }
                  } catch (_) {}
                  break

                case "requires_url":
                  // Handle URL requirement
                  addNewAssistantMessage("I need to analyze a specific website to build this extension properly. Let me get that information from you...")
                  
                  // Store the current request info for URL continuation
                  currentRequestRef.current = {
                    prompt: prompt,
                    requestType: requestType,
                    projectId: projectId
                  }
                  
                  // Trigger URL prompt modal
                  setUrlPromptData({
                    data: {
                      requiresUrl: true,
                      message: "This extension would benefit from analyzing specific website structure. Please choose how you'd like to proceed.",
                      detectedSites: [],
                      detectedUrls: [],
                      featureRequest: prompt,
                      requestType: requestType
                    },
                    originalPrompt: prompt
                  })
                  setShowUrlPrompt(true)
                  break

                case "error":
                  addNewAssistantMessage("I encountered an error: " + data.content + "\n\nPlease try again or let me know if you need help with something else.")
                  break

                case "done":
                  // Emit the final explanation once when stream completes
                  if (explanationBufferRef.current.trim()) {
                    addNewAssistantMessage("Here's what I've built for you:\n\n" + explanationBufferRef.current.trim())
                    explanationBufferRef.current = ""
                  }

                  // Mark that code has been generated
                  if (!hasGeneratedCode) {
                    setHasGeneratedCode(true)
                  }

                  if (onCodeGenerated) {
                    onCodeGenerated({ success: true })
                  }

                  // Call auto-generate complete callback if this was an auto-generation
                  if (autoGeneratePrompt && onAutoGenerateComplete) {
                    onAutoGenerateComplete()
                  }
                  
                  // Reset message tracking
                  currentAssistantMessageRef.current = null
                // Cancel any active typing and render full responses immediately
                setTypingCancelSignal((v) => v + 1)
                  // Also request manifest focus
                  try {
                    if (typeof window !== 'undefined') {
                      const evt = new CustomEvent('editor:focusManifest')
                      window.dispatchEvent(evt)
                    }
                  } catch (_) {}
                  // Flush model thinking panel
                  setModelThinkingDisplay(prev => (modelThinkingFull || prev))
                  if (thinkingTimerRef.current) { clearTimeout(thinkingTimerRef.current); thinkingTimerRef.current = null }
                  break
              }
            } catch (parseError) {
              console.error('Error parsing stream data:', parseError)
            }
          }
        }
      }

    } catch (error) {
      console.error("Error in streaming generation:", error)
      const errorMessage = {
        role: "assistant",
        content: `I encountered an error: ${error.message}\n\nPlease try again or let me know if you need help with something else.`,
      }
      setMessages(prev => [...prev, errorMessage])
    } finally {
      setIsGenerating(false)
      currentAssistantMessageRef.current = null
      // Ensure any typing is cancelled on end/error
      setTypingCancelSignal((v) => v + 1)
      
      if (onGenerationEnd) {
        onGenerationEnd()
      }
    }
  }

  const startGenerationWithUrl = async (prompt, userUrl, requestType, projectId) => {
    if (isGenerating) return
    
    setIsGenerating(true)
    
    // Reset current assistant message tracking
    currentAssistantMessageRef.current = null
    thinkingBufferRef.current = ""
    thinkingMessageIndexRef.current = null

    if (onGenerationStart) {
      onGenerationStart()
    }

    try {
      // Start streaming response with URL
      const hasPrev = Boolean(previousResponseIdRef.current)
      const pathUsed = requestType === REQUEST_TYPES.ADD_TO_EXISTING && hasPrev ? 'responses_api' : 'manual_file_context'

      const response = await fetch("/api/generate/stream", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          prompt: prompt,
          projectId: projectId,
          requestType: requestType,
          userProvidedUrl: userUrl,
          skipScraping: false,
          previousResponseId: previousResponseIdRef.current,
          conversationTokenTotal,
          modelOverride
        }),
      })

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }

      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let buffer = ""

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || ""

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            try {
              const data = JSON.parse(line.slice(6))

              // Helper function to add new assistant message
              const addNewAssistantMessage = (content) => {
                const newMessage = {
                  role: "assistant",
                  content: content
                }
                setMessages(prev => [...prev, newMessage])
              }

              switch (data.type) {
                case "start":
                  console.log("ðŸš€ Stream started")
                  addNewAssistantMessage("Starting to analyze your request...")
                  break

                // Ignore intermediate status noise
                case "analyzing":
                case "analysis_complete":
                case "fetching_apis":
                case "apis_ready":
                case "scraping":
                case "scraping_complete":
                case "scraping_skipped":
                case "context_ready":
                case "generation_starting":
                  break

                case "phase":
                  // Suppress phase updates
                  break

                case "explanation":
                  // Buffer explanation tokens; emit at done
                  if (data.content) {
                    explanationBufferRef.current += data.content
                  }
                  break

                case "generating_code":
                  break

                case "code":
                case "files_saved":
                case "generation_complete":
                  // Skip additional noise
                  break

                case "requires_url":
                  // This shouldn't happen in URL mode, but handle gracefully
                  addNewAssistantMessage("Continuing with extension generation...")
                  break

                case "error":
                  addNewAssistantMessage("I encountered an error: " + data.content + "\n\nPlease try again or let me know if you need help with something else.")
                  break

                case "done":
                  // Emit final explanation once when stream completes
                  if (explanationBufferRef.current.trim()) {
                    addNewAssistantMessage("Here's what I've built for you:\n\n" + explanationBufferRef.current.trim())
                    explanationBufferRef.current = ""
                  }

                  // Mark that code has been generated
                  if (!hasGeneratedCode) {
                    setHasGeneratedCode(true)
                  }

                  if (onCodeGenerated) {
                    onCodeGenerated({ success: true })
                  }

                  // Call auto-generate complete callback if this was an auto-generation
                  if (autoGeneratePrompt && onAutoGenerateComplete) {
                    onAutoGenerateComplete()
                  }
                  
                  // Reset message tracking
                  currentAssistantMessageRef.current = null
                  break
              }
            } catch (parseError) {
              console.error('Error parsing stream data:', parseError)
            }
          }
        }
      }

    } catch (error) {
      console.error("Error in streaming generation with URL:", error)
      const errorMessage = {
        role: "assistant",
        content: `I encountered an error: ${error.message}\n\nPlease try again or let me know if you need help with something else.`,
      }
      setMessages(prev => [...prev, errorMessage])
    } finally {
      setIsGenerating(false)
      currentAssistantMessageRef.current = null
      currentRequestRef.current = null
      // Ensure any typing is cancelled on end/error
      setTypingCancelSignal((v) => v + 1)
      
      if (onGenerationEnd) {
        onGenerationEnd()
      }
    }
  }

  const handleSendMessage = async (e) => {
    e.preventDefault()
    if (!inputMessage.trim() || isGenerating) return

    const userMessage = {
      role: "user",
      content: inputMessage,
    }

    setMessages((prev) => [...prev, userMessage])
    const prompt = inputMessage
    setInputMessage("")
    
    // Use the same startGeneration function (false = manual generation)
    await startGeneration(prompt, false)
  }

  const handleUrlSubmit = async (data, userUrl, originalPrompt) => {
    setShowUrlPrompt(false)
    setUrlPromptData(null)
    
    // Continue generation with or without URL using the stored request info
    const requestInfo = currentRequestRef.current
    if (requestInfo) {
      if (userUrl === null) {
        // User chose to skip scraping; call streaming API with skipScraping=true
        try {
          setIsGenerating(true)
          if (onGenerationStart) {
            onGenerationStart()
          }

          const response = await fetch("/api/generate/stream", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              prompt: requestInfo.prompt,
              projectId: requestInfo.projectId,
              requestType: requestInfo.requestType,
              userProvidedUrl: null,
              skipScraping: true
            }),
          })

          if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`)
          }

          const reader = response.body.getReader()
          const decoder = new TextDecoder()
          let buffer = ""

          while (true) {
            const { done, value } = await reader.read()
            if (done) break

            buffer += decoder.decode(value, { stream: true })
            const lines = buffer.split('\n')
            buffer = lines.pop() || ""

            for (const line of lines) {
              if (line.startsWith('data: ')) {
                try {
                  const data = JSON.parse(line.slice(6))
                  switch (data.type) {
                    case "start":
                      // show concise transition
                      setMessages(prev => [...prev, { role: "assistant", content: "Analysis complete, now generating your extension...\nContinuing with extension generation..." }])
                      break
                    case "explanation":
                      if (data.content) {
                        explanationBufferRef.current += data.content
                      }
                      break
                    case "done":
                      if (explanationBufferRef.current.trim()) {
                        setMessages(prev => [...prev, { role: "assistant", content: "Here's what I've built for you:\n\n" + explanationBufferRef.current.trim() }])
                        explanationBufferRef.current = ""
                      }
                      if (!hasGeneratedCode) {
                        setHasGeneratedCode(true)
                      }
                      if (onCodeGenerated) {
                        onCodeGenerated({ success: true })
                      }
                      break
                    default:
                      // ignore other noise
                      break
                  }
                } catch (e) {
                  console.error('Error parsing stream data (skipScraping):', e)
                }
              }
            }
          }
        } catch (err) {
          console.error('Error continuing generation without URL:', err)
          setMessages(prev => [...prev, { role: "assistant", content: `I encountered an error: ${err.message}` }])
        } finally {
          setIsGenerating(false)
          currentAssistantMessageRef.current = null
          currentRequestRef.current = null
          // Ensure any typing is cancelled on end/error
          setTypingCancelSignal((v) => v + 1)
          if (onGenerationEnd) {
            onGenerationEnd()
          }
        }
      } else {
        await startGenerationWithUrl(requestInfo.prompt, userUrl, requestInfo.requestType, requestInfo.projectId)
      }
    }
  }

  const handleUrlCancel = () => {
    setShowUrlPrompt(false)
    setUrlPromptData(null)
    setIsGenerating(false)
    currentAssistantMessageRef.current = null
  }


  return (
    <div className="flex flex-col h-full">
      {/* Chat Header */}
      <div className="p-4 border-b border-white/10">
        <p className="text-sm text-gray-400">
          {projectName || "describe what you want to add or modify"}
        </p>
      </div>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto custom-scrollbar p-4 space-y-4 bg-gradient-to-b from-slate-800/30 to-slate-900/30">
        {messages
          .filter((message) => !message.isThinking)
          .map((message, index) => (
            <ChatMessage key={index} message={message} index={index} typingCancelSignal={typingCancelSignal} />
          ))}
        
        {/* Show typing indicator when generating */}
        {isGenerating && (
          <div className="flex justify-start">
            <div className="max-w-[80%] p-3 rounded-lg bg-slate-700/50 text-gray-200 border border-slate-600">
              <div className="flex items-center space-x-2">
                <div className="flex space-x-1">
                  <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0ms' }}></div>
                  <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '150ms' }}></div>
                  <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '300ms' }}></div>
                </div>
                <span className="text-sm text-gray-400">thinking...</span>
              </div>
            </div>
          </div>
        )}
        
        {/* Collapsible Model Thinking Panel (Gemini) */}
        {(() => {
          const hasContent = !!(modelThinkingDisplay || modelThinkingFull)
          const forceShow = thinkingChunkCountRef.current > 0 // Force show if we have chunks
          const shouldShow = hasContent || forceShow
          console.log('[streaming-chat] thinking panel render check:', {
            hasContent,
            forceShow,
            shouldShow,
            modelThinkingDisplay: modelThinkingDisplay?.length || 0,
            modelThinkingFull: modelThinkingFull?.length || 0,
            chunkCount: thinkingChunkCountRef.current
          })
          return shouldShow
        })() && (
          <div className="mt-2">
            <button
              type="button"
              className="flex items-center justify-between w-full text-left text-xs uppercase tracking-wide text-slate-300 bg-slate-800/40 hover:bg-slate-800/60 border border-slate-600/40 px-3 py-2 rounded"
              onClick={() => setIsModelThinkingOpen(!isModelThinkingOpen)}
              aria-expanded={isModelThinkingOpen}
            >
              <span>Model thoughts ({thinkingChunkCountRef.current} chunks)</span>
              {isModelThinkingOpen ? <ChevronDown className="h-3 w-3" /> : <ChevronRight className="h-3 w-3" />}
            </button>
            {isModelThinkingOpen && (
              <div className="mt-2 p-3 rounded-lg border border-yellow-500/20 bg-yellow-900/10 text-yellow-200 text-sm whitespace-pre-wrap leading-relaxed max-h-48 overflow-auto font-mono">
                {modelThinkingDisplay || modelThinkingFull || `Debug: No content yet. Chunks: ${thinkingChunkCountRef.current}`}
              </div>
            )}
          </div>
        )}

        <div ref={messagesEndRef} />
      </div>

      {/* Input */}
      <div className="p-4 border-t border-white/10">
        <form onSubmit={handleSendMessage} className="space-y-3">
          <Textarea
            value={inputMessage}
            onChange={(e) => setInputMessage(e.target.value)}
            placeholder={projectName ? `describe what you want to add or modify in ${projectName}...` : "describe what you want to add or modify..."}
            className="min-h-[80px] bg-slate-700/50 border-slate-600 text-white placeholder:text-slate-400 resize-none"
          />
          <Button
            type="submit"
            disabled={!inputMessage.trim() || isGenerating || !projectId}
            className="w-full bg-gradient-to-r from-purple-500 to-blue-500 hover:from-purple-600 hover:to-blue-600 disabled:opacity-50"
          >
            {isGenerating ? (
              <>
                <Loader2 className="h-4 w-4 mr-2 animate-spin" />
                generating...
              </>
            ) : !projectId ? (
              "setting up project..."
            ) : (
              <>
                <Send className="h-4 w-4 mr-2" />
                send
              </>
            )}
          </Button>
        </form>
      </div>

      {/* URL Prompt Modal */}
      {showUrlPrompt && urlPromptData && (
        <ModalUrlPrompt
          data={urlPromptData.data}
          originalPrompt={urlPromptData.originalPrompt}
          onUrlSubmit={handleUrlSubmit}
          onCancel={handleUrlCancel}
          onCodeGenerated={onCodeGenerated}
          projectId={projectId}
          hasGeneratedCode={hasGeneratedCode}
          onGenerationEnd={onGenerationEnd}
        />
      )}
    </div>
  )
}
