"use client"

import { useState, useRef, useEffect } from "react"
import { Button } from "@/components/ui/button"
import { Textarea } from "@/components/ui/forms-and-input/textarea"
import { Send, Loader2, ChevronDown, ChevronRight } from "lucide-react"
import { REQUEST_TYPES } from "@/lib/prompts/request-types"
import ModalUrlPrompt from "@/components/ui/modals/modal-url-prompt"
import ModalApiPrompt from "@/components/ui/modals/modal-api-prompt"
import ChatMessage from "@/components/ui/chat/chat-message"

export default function StreamingChat({ 
  projectId, 
  projectName, 
  autoGeneratePrompt, 
  onAutoGenerateComplete, 
  onCodeGenerated, 
  onGenerationStart, 
  onGenerationEnd, 
  isProjectReady,
  modelOverride
}) {
  const [inputMessage, setInputMessage] = useState("")
  const [messages, setMessages] = useState([
    {
      role: "assistant",
      content: "hi! i'm **chromie**, your chrome extension assistant. tell me what you'd like in your extension.",
    },
  ])
  const [isGenerating, setIsGenerating] = useState(false)
  const [hasGeneratedCode, setHasGeneratedCode] = useState(false)
  const [urlPromptData, setUrlPromptData] = useState(null)
  const [showUrlPrompt, setShowUrlPrompt] = useState(false)
  const [apiPromptData, setApiPromptData] = useState(null)
  const [showApiPrompt, setShowApiPrompt] = useState(false)
  const messagesEndRef = useRef(null)
  const autoGeneratedRef = useRef(false)
  const currentRequestRef = useRef(null)
  const currentAssistantMessageRef = useRef(null) // Track current streaming message
  const thinkingBufferRef = useRef("") // Buffer for accumulating thinking content
  const thinkingMessageIndexRef = useRef(null) // Track the thinking message index
  const explanationBufferRef = useRef("") // Buffer for accumulating final explanation tokens
  const sentGeneratingMessageRef = useRef(false) // Ensure generating message shows only once
  const previousResponseIdRef = useRef(null)
  const [conversationTokenTotal, setConversationTokenTotal] = useState(0)
  const [typingCancelSignal, setTypingCancelSignal] = useState(0)
  // Gemini thinking panel state (per request)
  const [isModelThinkingOpen, setIsModelThinkingOpen] = useState(false)
  const [modelThinkingFull, setModelThinkingFull] = useState("")
  const [modelThinkingDisplay, setModelThinkingDisplay] = useState("")
  const [isGenerationComplete, setIsGenerationComplete] = useState(false)
  const thinkingTimerRef = useRef(null)
  const thinkingTokensRef = useRef([])
  const thinkingIdxRef = useRef(0)
  const thinkingChunkCountRef = useRef(0)
  // Planning progress state
  const [planningProgress, setPlanningProgress] = useState(null)
  const [currentPlanningPhase, setCurrentPlanningPhase] = useState(null)

  // Reset local-only conversation state on project change (navigation/refresh)
  useEffect(() => {
    previousResponseIdRef.current = null
    setConversationTokenTotal(0)
  }, [projectId])

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" })
  }

  // Debug logging
  useEffect(() => {
    // Component mounted
  }, [projectId, isProjectReady])

  // No cleanup needed since we removed buffer timeout

  // Only scroll to bottom when assistant messages are added or streaming content changes
  useEffect(() => {
    if (messages.length > 0) {
      const lastMessage = messages[messages.length - 1]
      if (lastMessage.role === 'assistant') {
        scrollToBottom()
      }
    }
  }, [messages])
  
  // Scroll when new messages are added
  useEffect(() => {
    scrollToBottom()
  }, [messages])

  // Adaptive typing for model thinking panel (slightly larger batches)
  useEffect(() => {
    if (!modelThinkingFull) return
    // Build token list on new text arrival
    const tokenize = (text) => {
      const sentences = text.split(/(?<=[.!?])\s+/)
      const result = []
      for (const s of sentences) {
        const parts = s.split(/(\s+)/)
        for (const p of parts) { if (p) result.push(p) }
        result.push(" ")
      }
      return result
    }
    thinkingTokensRef.current = tokenize(modelThinkingFull)
    const total = thinkingTokensRef.current.length
    const step = () => {
      const base = total < 40 ? 2 : total < 120 ? 3 : total < 250 ? 5 : 8
      const next = Math.min(thinkingIdxRef.current + base, total)
      const slice = thinkingTokensRef.current.slice(0, next).join("")
      thinkingIdxRef.current = next
      setModelThinkingDisplay(slice)
      if (next < total) {
        const delay = total < 40 ? 28 : total < 120 ? 18 : total < 250 ? 12 : 9
        thinkingTimerRef.current = setTimeout(step, delay)
      }
    }
    if (thinkingTimerRef.current) clearTimeout(thinkingTimerRef.current)
    thinkingTimerRef.current = setTimeout(step, 8)
    return () => { if (thinkingTimerRef.current) clearTimeout(thinkingTimerRef.current) }
  }, [modelThinkingFull])

  useEffect(() => {
    if (autoGeneratePrompt && isProjectReady && !hasGeneratedCode && !isGenerating && !autoGeneratedRef.current) {
      autoGeneratedRef.current = true // Prevent duplicate auto-generation
      setInputMessage(autoGeneratePrompt)
      
      // Add the user message immediately
      const userMessage = {
        role: "user",
        content: autoGeneratePrompt,
      }
      setMessages((prev) => [...prev, userMessage])
      
      // Start the generation process
      startGeneration(autoGeneratePrompt, true) // true indicates this is auto-generation
    }
  }, [autoGeneratePrompt, isProjectReady, hasGeneratedCode, isGenerating])

  const startGeneration = async (prompt, isAutoGeneration = false) => {
    if (isGenerating) {
      console.log('⚠️ [StreamingChat] startGeneration called but already generating - ignoring')
      return
    }
    
    console.log('🚀 [StreamingChat] Starting generation:', { prompt: prompt?.substring(0, 50), isAutoGeneration })
    setIsGenerating(true)
    
    // Reset current assistant message tracking
    currentAssistantMessageRef.current = null
    thinkingBufferRef.current = ""
    thinkingMessageIndexRef.current = null
    explanationBufferRef.current = ""
    sentGeneratingMessageRef.current = false
    // Reset model thinking panel per request
    setIsModelThinkingOpen(false)
    setModelThinkingFull("")
    setModelThinkingDisplay("")
    setIsGenerationComplete(false)
    thinkingTokensRef.current = []
    thinkingIdxRef.current = 0
    thinkingChunkCountRef.current = 0
    if (thinkingTimerRef.current) { clearTimeout(thinkingTimerRef.current); thinkingTimerRef.current = null }
    // Reset planning progress state
    setPlanningProgress(null)
    setCurrentPlanningPhase(null)

    if (onGenerationStart) {
      onGenerationStart()
    }

    try {
      // Force refresh hasGeneratedCode from Supabase before determining request type
      let currentHasGeneratedCode = hasGeneratedCode
      if (projectId) {
        try {
          const response = await fetch(`/api/projects/${projectId}/has-generated-code`)
          if (response.ok) {
            const data = await response.json()
            currentHasGeneratedCode = data.hasGeneratedCode
            if (data.hasGeneratedCode !== hasGeneratedCode) {
              setHasGeneratedCode(data.hasGeneratedCode)
            }
          }
        } catch (error) {
          console.error('Error refreshing hasGeneratedCode:', error)
        }
      }
      
      const requestType = currentHasGeneratedCode ? REQUEST_TYPES.ADD_TO_EXISTING : REQUEST_TYPES.NEW_EXTENSION
      const hasPrev = Boolean(previousResponseIdRef.current)
      const pathUsed = requestType === REQUEST_TYPES.ADD_TO_EXISTING && hasPrev ? 'responses_api' : 'manual_file_context'

      // Start streaming response
      const response = await fetch("/api/generate/stream", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          prompt: prompt,
          projectId,
          requestType: requestType,
          previousResponseId: previousResponseIdRef.current,
          conversationTokenTotal,
          modelOverride
        }),
      })

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }

      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let buffer = ""

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || ""

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            try {
              const data = JSON.parse(line.slice(6))

              // Helper function to add new assistant message
              const addNewAssistantMessage = (content) => {
                const newMessage = {
                  role: "assistant",
                  content: content
                }
                setMessages(prev => [...prev, newMessage])
              }

              switch (data.type) {
                case "thinking_chunk":
                case "thinking":
                  // Append Gemini thinking text to model thinking panel
                  if (typeof data.content === 'string' && data.content.length > 0) {
                    thinkingChunkCountRef.current += 1
                    setModelThinkingFull(prev => {
                      const newContent = prev + data.content
                      return newContent
                    })
                    // Keep thinking panel collapsed by default; user can expand
                  }
                  break
                case "planning_progress":
                  // Handle planning progress updates
                  if (data.phase && data.content) {
                    setCurrentPlanningPhase(data.phase)
                    setPlanningProgress(data.content)
                  }
                  break
                case "start":
                  // Only add the start message if we haven't generated code yet
                  if (!hasGeneratedCode) {
                    addNewAssistantMessage("Starting to analyze your request...")
                  }
                  break
                case "token_usage":
                  if (typeof data.total === 'number') {
                    setConversationTokenTotal(data.total)
                  }
                  break
                case "usage_summary":
                  try {
                    const t = typeof data.thinking_tokens === 'number' ? data.thinking_tokens : null
                    const c = typeof data.completion_tokens === 'number' ? data.completion_tokens : null
                    const lim = typeof data.token_limit === 'number' ? data.token_limit : null
                    const parts = []
                    if (t !== null) parts.push(`thinking tokens: ${t}`)
                    if (c !== null) parts.push(`completion tokens: ${c}`)
                    if (lim !== null) parts.push(`token limit: ${lim}`)
                    if (parts.length > 0) {
                      addNewAssistantMessage(`usage: ${parts.join(', ')}`)
                    }
                  } catch (_) {}
                  break
                case "context_window":
                  addNewAssistantMessage('Context limit reached. Please start a new conversation.')
                  if (typeof data.total === 'number') {
                    setConversationTokenTotal(data.total)
                  }
                  // Do not continue processing further
                  break
                case "response_id":
                  previousResponseIdRef.current = data.id
                  if (typeof data.tokensUsedThisRequest === 'number') {
                    const nextTotal = (conversationTokenTotal || 0) + data.tokensUsedThisRequest
                    setConversationTokenTotal(nextTotal)
                  }
                  break

                // Ignore intermediate status noise
                case "analyzing":
                case "analysis_complete":
                case "fetching_apis":
                case "apis_ready":
                case "scraping":
                case "scraping_complete":
                case "scraping_skipped":
                case "context_ready":
                case "generation_starting":
                  break

                case "phase":
                  // Do not render phase updates to keep UI clean
                  break

                case "explanation":
                  // Buffer explanation tokens; emit once on done
                  if (data.content) {
                    explanationBufferRef.current += data.content
                  }
                  break

                case "generating_code":
                  break

                case "code":
                  // If backend supplies file path info, auto-select in editor via global event
                  try {
                    const filePath = data.file_path || data.path || data.file || null
                    if (filePath && typeof window !== 'undefined') {
                      const evt = new CustomEvent('editor:selectFile', { detail: { file_path: String(filePath) } })
                      window.dispatchEvent(evt)
                    }
                  } catch (_) {}
                  break
                case "files_saved":
                case "generation_complete":
                  // On save or completion, try to focus manifest.json after a short delay
                  try {
                    if (typeof window !== 'undefined') {
                      setTimeout(() => {
                        const evt = new CustomEvent('editor:focusManifest')
                        window.dispatchEvent(evt)
                      }, 200)
                    }
                  } catch (_) {}
                  break

                case "requires_url":
                  // Handle URL requirement
                  console.log('📋 Received requires_url signal:', data)
                  addNewAssistantMessage("I need to analyze a specific website to build this extension properly. Let me get that information from you...")
                  
                  // Store the current request info for URL continuation
                  currentRequestRef.current = {
                    prompt: prompt,
                    requestType: requestType,
                    projectId: projectId,
                    // Preserve analysis data so we can resume without re-planning
                    analysisData: data.analysisData
                  }
                  
                  // Trigger URL prompt modal with detected sites from the response
                  setUrlPromptData({
                    data: {
                      requiresUrl: true,
                      message: data.content || "This extension would benefit from analyzing specific website structure. Please choose how you'd like to proceed.",
                      detectedSites: data.detectedSites || [],
                      detectedUrls: data.detectedUrls || [],
                      featureRequest: prompt,
                      requestType: requestType
                    },
                    originalPrompt: prompt
                  })
                  setShowUrlPrompt(true)
                  console.log('✅ URL prompt modal should now be visible')
                  break

                case "requires_api":
                  // Handle API requirement
                  console.log('🔌 Received requires_api signal:', data)
                  addNewAssistantMessage("This extension looks like it might need external APIs. Let me get endpoint details...")
                  
                  // Store the current request info for API continuation
                  currentRequestRef.current = {
                    prompt: prompt,
                    requestType: requestType,
                    projectId: projectId,
                    analysisData: data.analysisData
                  }
                  
                  // Trigger API prompt modal with suggested APIs from the response
                  setApiPromptData({
                    data: {
                      suggestedAPIs: data.suggestedAPIs || [],
                      message: data.content || "This extension looks like it might need external API endpoints. Please configure them or choose to skip."
                    },
                    originalPrompt: prompt
                  })
                  setShowApiPrompt(true)
                  console.log('✅ API prompt modal should now be visible')
                  break

                case "error":
                  addNewAssistantMessage("I encountered an error: " + data.content + "\n\nPlease try again or let me know if you need help with something else.")
                  break

                case "done":
                  // Emit the final explanation once when stream completes
                  if (explanationBufferRef.current.trim()) {
                    addNewAssistantMessage("Here's what I've built for you:\n\n" + explanationBufferRef.current.trim())
                    explanationBufferRef.current = ""
                  }

                  // Mark generation as complete to hide model thoughts
                  setIsGenerationComplete(true)
                  // Clear planning progress when generation is complete
                  setPlanningProgress(null)
                  setCurrentPlanningPhase(null)

                  // Mark that code has been generated
                  if (!hasGeneratedCode) {
                    setHasGeneratedCode(true)
                  }

                  if (onCodeGenerated) {
                    onCodeGenerated({ success: true })
                  }

                  // Call auto-generate complete callback if this was an auto-generation
                  if (autoGeneratePrompt && onAutoGenerateComplete) {
                    onAutoGenerateComplete()
                  }
                  
                  // Clear input message after auto-generation completes
                  if (autoGeneratePrompt) {
                    setInputMessage("")
                  }
                  
                  // Reset message tracking
                  currentAssistantMessageRef.current = null
                // Cancel any active typing and render full responses immediately
                setTypingCancelSignal((v) => v + 1)
                  // Also request manifest focus
                  try {
                    if (typeof window !== 'undefined') {
                      const evt = new CustomEvent('editor:focusManifest')
                      window.dispatchEvent(evt)
                    }
                  } catch (_) {}
                  // Flush model thinking panel
                  setModelThinkingDisplay(prev => (modelThinkingFull || prev))
                  if (thinkingTimerRef.current) { clearTimeout(thinkingTimerRef.current); thinkingTimerRef.current = null }
                  break
              }
            } catch (parseError) {
              console.error('Error parsing stream data:', parseError)
            }
          }
        }
      }

    } catch (error) {
      console.error("Error in streaming generation:", error)
      const errorMessage = {
        role: "assistant",
        content: `I encountered an error: ${error.message}\n\nPlease try again or let me know if you need help with something else.`,
      }
      setMessages(prev => [...prev, errorMessage])
    } finally {
      setIsGenerating(false)
      currentAssistantMessageRef.current = null
      // Ensure any typing is cancelled on end/error
      setTypingCancelSignal((v) => v + 1)
      
      // Clear input message after auto-generation completes (even on error)
      if (autoGeneratePrompt) {
        setInputMessage("")
      }
      
      if (onGenerationEnd) {
        onGenerationEnd()
      }
    }
  }

  const startGenerationWithUrl = async (prompt, userUrl, requestType, projectId) => {
    if (isGenerating) return
    
    setIsGenerating(true)
    
    // Reset current assistant message tracking
    currentAssistantMessageRef.current = null
    thinkingBufferRef.current = ""
    thinkingMessageIndexRef.current = null
    // Reset planning progress state
    setPlanningProgress(null)
    setCurrentPlanningPhase(null)

    if (onGenerationStart) {
      onGenerationStart()
    }

    try {
      // Start streaming response with URL
      const hasPrev = Boolean(previousResponseIdRef.current)
      const pathUsed = requestType === REQUEST_TYPES.ADD_TO_EXISTING && hasPrev ? 'responses_api' : 'manual_file_context'

      const response = await fetch("/api/generate/stream", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          prompt: prompt,
          projectId: projectId,
          requestType: requestType,
          userProvidedUrl: userUrl,
          skipScraping: false,
          previousResponseId: previousResponseIdRef.current,
          conversationTokenTotal,
          modelOverride
        }),
      })

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }

      const reader = response.body.getReader()
      const decoder = new TextDecoder()
      let buffer = ""

      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || ""

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            try {
              const data = JSON.parse(line.slice(6))

              // Helper function to add new assistant message
              const addNewAssistantMessage = (content) => {
                const newMessage = {
                  role: "assistant",
                  content: content
                }
                setMessages(prev => [...prev, newMessage])
              }

              switch (data.type) {
                case "start":
                  // Only add the start message if we haven't generated code yet
                  if (!hasGeneratedCode) {
                    addNewAssistantMessage("Starting to analyze your request...")
                  }
                  break
                case "planning_progress":
                  // Handle planning progress updates
                  if (data.phase && data.content) {
                    setCurrentPlanningPhase(data.phase)
                    setPlanningProgress(data.content)
                  }
                  break
                case "usage_summary":
                  try {
                    const t = typeof data.thinking_tokens === 'number' ? data.thinking_tokens : null
                    const c = typeof data.completion_tokens === 'number' ? data.completion_tokens : null
                    const lim = typeof data.token_limit === 'number' ? data.token_limit : null
                    const parts = []
                    if (t !== null) parts.push(`thinking tokens: ${t}`)
                    if (c !== null) parts.push(`completion tokens: ${c}`)
                    if (lim !== null) parts.push(`token limit: ${lim}`)
                    if (parts.length > 0) {
                      addNewAssistantMessage(`usage: ${parts.join(', ')}`)
                    }
                  } catch (_) {}
                  break

                // Ignore intermediate status noise
                case "analyzing":
                case "analysis_complete":
                case "fetching_apis":
                case "apis_ready":
                case "scraping":
                case "scraping_complete":
                case "scraping_skipped":
                case "context_ready":
                case "generation_starting":
                  break

                case "phase":
                  // Suppress phase updates
                  break

                case "explanation":
                  // Buffer explanation tokens; emit at done
                  if (data.content) {
                    explanationBufferRef.current += data.content
                  }
                  break

                case "generating_code":
                  break

                case "code":
                case "files_saved":
                case "generation_complete":
                  // Skip additional noise
                  break

                case "requires_url":
                  // This shouldn't happen in URL mode, but handle gracefully
                  addNewAssistantMessage("Continuing with extension generation...")
                  break

                case "requires_api":
                  // Handle API requirement after URL flow
                  console.log('🔌 [URL flow] Received requires_api signal:', data)
                  addNewAssistantMessage("This extension needs external API details. Let me get those from you...")
                  
                  // Store the current request info for API continuation
                  currentRequestRef.current = {
                    prompt: prompt,
                    requestType: requestType,
                    projectId: projectId,
                    analysisData: data.analysisData
                  }
                  
                  // Trigger API prompt modal
                  setApiPromptData({
                    data: {
                      suggestedAPIs: data.suggestedAPIs || [],
                      message: data.content || "This extension needs external API endpoints. Please configure them or choose to skip."
                    },
                    originalPrompt: prompt
                  })
                  setShowApiPrompt(true)
                  console.log('✅ [URL flow] API prompt modal should now be visible')
                  break

                case "error":
                  addNewAssistantMessage("I encountered an error: " + data.content + "\n\nPlease try again or let me know if you need help with something else.")
                  break

                case "done":
                  // Emit final explanation once when stream completes
                  if (explanationBufferRef.current.trim()) {
                    addNewAssistantMessage("Here's what I've built for you:\n\n" + explanationBufferRef.current.trim())
                    explanationBufferRef.current = ""
                  }

                  // Mark generation as complete to hide model thoughts
                  setIsGenerationComplete(true)
                  // Clear planning progress when generation is complete
                  setPlanningProgress(null)
                  setCurrentPlanningPhase(null)

                  // Mark that code has been generated
                  if (!hasGeneratedCode) {
                    setHasGeneratedCode(true)
                  }

                  if (onCodeGenerated) {
                    onCodeGenerated({ success: true })
                  }

                  // Call auto-generate complete callback if this was an auto-generation
                  if (autoGeneratePrompt && onAutoGenerateComplete) {
                    onAutoGenerateComplete()
                  }
                  
                  // Clear input message after auto-generation completes
                  if (autoGeneratePrompt) {
                    setInputMessage("")
                  }
                  
                  // Reset message tracking
                  currentAssistantMessageRef.current = null
                  break
              }
            } catch (parseError) {
              console.error('Error parsing stream data:', parseError)
            }
          }
        }
      }

    } catch (error) {
      console.error("Error in streaming generation with URL:", error)
      const errorMessage = {
        role: "assistant",
        content: `I encountered an error: ${error.message}\n\nPlease try again or let me know if you need help with something else.`,
      }
      setMessages(prev => [...prev, errorMessage])
    } finally {
      setIsGenerating(false)
      currentAssistantMessageRef.current = null
      currentRequestRef.current = null
      // Ensure any typing is cancelled on end/error
      setTypingCancelSignal((v) => v + 1)
      
      if (onGenerationEnd) {
        onGenerationEnd()
      }
    }
  }

  const handleSendMessage = async (e) => {
    e.preventDefault()
    if (!inputMessage.trim() || isGenerating) return

    const userMessage = {
      role: "user",
      content: inputMessage,
    }

    setMessages((prev) => [...prev, userMessage])
    const prompt = inputMessage
    setInputMessage("")
    
    // Use the same startGeneration function (false = manual generation)
    await startGeneration(prompt, false)
  }

  const handleUrlSubmit = async (data, userUrl, originalPrompt) => {
    setShowUrlPrompt(false)
    setUrlPromptData(null)
    
    // Continue generation with or without URL using the stored request info
    const requestInfo = currentRequestRef.current
    if (requestInfo) {
      if (userUrl === null) {
        // User chose to skip scraping; call streaming API with skipScraping=true
        try {
          setIsGenerating(true)
          if (onGenerationStart) {
            onGenerationStart()
          }

          const response = await fetch("/api/generate/stream", {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              prompt: requestInfo.prompt,
              projectId: requestInfo.projectId,
              requestType: requestInfo.requestType,
              userProvidedUrl: null,
              skipScraping: true
            }),
          })

          if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`)
          }

          const reader = response.body.getReader()
          const decoder = new TextDecoder()
          let buffer = ""

          while (true) {
            const { done, value } = await reader.read()
            if (done) break

            buffer += decoder.decode(value, { stream: true })
            const lines = buffer.split('\n')
            buffer = lines.pop() || ""

            for (const line of lines) {
              if (line.startsWith('data: ')) {
                try {
                  const data = JSON.parse(line.slice(6))
                  switch (data.type) {
                    case "start":
                      // show concise transition
                      setMessages(prev => [...prev, { role: "assistant", content: "Analysis complete, now generating your extension...\nContinuing with extension generation..." }])
                      break
                    case "planning_progress":
                      // Handle planning progress updates
                      if (data.phase && data.content) {
                        setCurrentPlanningPhase(data.phase)
                        setPlanningProgress(data.content)
                      }
                      break
                    case "explanation":
                      if (data.content) {
                        explanationBufferRef.current += data.content
                      }
                      break
                        case "usage_summary":
                      try {
                        const t = typeof data.thinking_tokens === 'number' ? data.thinking_tokens : null
                        const c = typeof data.completion_tokens === 'number' ? data.completion_tokens : null
                            const lim = typeof data.token_limit === 'number' ? data.token_limit : null
                        const parts = []
                        if (t !== null) parts.push(`thinking tokens: ${t}`)
                        if (c !== null) parts.push(`completion tokens: ${c}`)
                            if (lim !== null) parts.push(`token limit: ${lim}`)
                        if (parts.length > 0) {
                          setMessages(prev => [...prev, { role: "assistant", content: `usage: ${parts.join(', ')}` }])
                        }
                      } catch (_) {}
                      break
                        case "requires_api":
                          // Handle API requirement when scraping was skipped
                          console.log('🔌 [Skip scraping] Received requires_api signal:', data)
                          currentRequestRef.current = {
                            prompt: requestInfo.prompt,
                            requestType: requestInfo.requestType,
                            projectId: requestInfo.projectId,
                            analysisData: data.analysisData
                          }
                          setApiPromptData({
                            data: {
                              suggestedAPIs: data.suggestedAPIs || [],
                              message: data.content || "This extension needs external API endpoints. Please configure them or choose to skip."
                            },
                            originalPrompt: requestInfo.prompt
                          })
                          setShowApiPrompt(true)
                          break
                    case "done":
                      if (explanationBufferRef.current.trim()) {
                        setMessages(prev => [...prev, { role: "assistant", content: "Here's what I've built for you:\n\n" + explanationBufferRef.current.trim() }])
                        explanationBufferRef.current = ""
                      }
                      // Mark generation as complete to hide model thoughts
                      setIsGenerationComplete(true)
                      // Clear planning progress when generation is complete
                      setPlanningProgress(null)
                      setCurrentPlanningPhase(null)
                      if (!hasGeneratedCode) {
                        setHasGeneratedCode(true)
                      }
                      if (onCodeGenerated) {
                        onCodeGenerated({ success: true })
                      }
                      break
                    default:
                      // ignore other noise
                      break
                  }
                } catch (e) {
                  console.error('Error parsing stream data (skipScraping):', e)
                }
              }
            }
          }
        } catch (err) {
          console.error('Error continuing generation without URL:', err)
          setMessages(prev => [...prev, { role: "assistant", content: `I encountered an error: ${err.message}` }])
        } finally {
          setIsGenerating(false)
          currentAssistantMessageRef.current = null
          currentRequestRef.current = null
          // Ensure any typing is cancelled on end/error
          setTypingCancelSignal((v) => v + 1)
          if (onGenerationEnd) {
            onGenerationEnd()
          }
        }
      } else {
        await startGenerationWithUrl(requestInfo.prompt, userUrl, requestInfo.requestType, requestInfo.projectId)
      }
    }
  }

  const handleUrlCancel = () => {
    setShowUrlPrompt(false)
    setUrlPromptData(null)
    setIsGenerating(false)
    currentAssistantMessageRef.current = null
  }

  const handleApiSubmit = async (data, userApis, originalPrompt) => {
    setShowApiPrompt(false)
    setApiPromptData(null)
    
    // Continue generation with APIs using the stored request info
    const requestInfo = currentRequestRef.current
    if (requestInfo) {
      try {
        setIsGenerating(true)
        if (onGenerationStart) {
          onGenerationStart()
        }

        const response = await fetch("/api/generate/stream", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            prompt: requestInfo.prompt,
            projectId: requestInfo.projectId,
            requestType: requestInfo.requestType,
            userProvidedApis: userApis,
            // Use analysis data if available (for resuming after API prompt)
            ...(requestInfo.analysisData && {
              initialRequirementsAnalysis: requestInfo.analysisData.requirements,
              initialPlanningTokenUsage: requestInfo.analysisData.tokenUsage
            })
          }),
        })

        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`)
        }

        const reader = response.body.getReader()
        const decoder = new TextDecoder()
        let buffer = ""

        while (true) {
          const { done, value } = await reader.read()
          if (done) break

          buffer += decoder.decode(value, { stream: true })
          const lines = buffer.split('\n')
          buffer = lines.pop() || ""

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              try {
                const data = JSON.parse(line.slice(6))
                await handleStreamChunk(data, originalPrompt)
              } catch (parseError) {
                console.error('Error parsing stream data:', parseError)
              }
            }
          }
        }
        
        // Ensure generation state is reset after completion
        setIsGenerating(false)
        if (onGenerationEnd) {
          onGenerationEnd()
        }
      } catch (error) {
        console.error('Error in API continuation:', error)
        addNewAssistantMessage("I encountered an error while continuing generation: " + error.message + "\n\nPlease try again.")
        setIsGenerating(false)
        if (onGenerationEnd) {
          onGenerationEnd()
        }
      }
    }
  }

  const handleApiCancel = () => {
    setShowApiPrompt(false)
    setApiPromptData(null)
    setIsGenerating(false)
    currentAssistantMessageRef.current = null
  }

  return (
    <div className="flex flex-col h-full">
      {/* Chat Header */}
      <div className="p-4 border-b border-white/10">
        <p className="text-sm text-gray-400">
          {projectName || "describe what you want to add or modify"}
        </p>
      </div>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto custom-scrollbar p-4 space-y-4 bg-gradient-to-b from-slate-800/30 to-slate-900/30">
        {messages
          .filter((message) => !message.isThinking)
          .map((message, index) => (
            <ChatMessage key={index} message={message} index={index} typingCancelSignal={typingCancelSignal} />
          ))}
        
        {/* Show typing indicator when generating */}
        {isGenerating && (
          <div className="flex justify-start">
            <div className="max-w-[80%] p-4 rounded-2xl bg-gradient-to-r from-purple-500/20 to-blue-500/20 backdrop-blur-sm border border-purple-400/30 shadow-lg">
              <div className="flex items-center space-x-3">
                <div className="flex space-x-1">
                  <div className="w-2 h-2 bg-purple-400 rounded-full animate-bounce" style={{ animationDelay: '0ms' }}></div>
                  <div className="w-2 h-2 bg-blue-400 rounded-full animate-bounce" style={{ animationDelay: '150ms' }}></div>
                  <div className="w-2 h-2 bg-purple-400 rounded-full animate-bounce" style={{ animationDelay: '300ms' }}></div>
                </div>
                <span className="text-sm text-white font-medium">thinking...</span>
              </div>
            </div>
          </div>
        )}

        {/* Show planning progress when available */}
        {planningProgress && currentPlanningPhase && (
          <div className="flex justify-start">
            <div className="max-w-[80%] p-4 rounded-2xl bg-gradient-to-r from-slate-600/20 to-slate-700/20 backdrop-blur-sm border border-slate-500/30 shadow-lg">
              <div className="flex items-center space-x-3">
                <div className="flex space-x-1">
                  <div className="w-2 h-2 bg-slate-400 rounded-full animate-pulse"></div>
                  <div className="w-2 h-2 bg-slate-300 rounded-full animate-pulse" style={{ animationDelay: '200ms' }}></div>
                  <div className="w-2 h-2 bg-slate-400 rounded-full animate-pulse" style={{ animationDelay: '400ms' }}></div>
                </div>
                <div className="flex flex-col">
                  <span className="text-xs text-slate-300 uppercase tracking-wide">
                    {currentPlanningPhase === 'analysis' && 'Planning'}
                    {currentPlanningPhase === 'documentation' && 'Documentation'}
                    {currentPlanningPhase === 'scraping' && 'Web Analysis'}
                    {!['analysis', 'documentation', 'scraping'].includes(currentPlanningPhase) && 'Planning'}
                  </span>
                  <span className="text-sm text-white font-medium">{planningProgress}</span>
                </div>
              </div>
            </div>
          </div>
        )}
        
        {/* Collapsible Model Thinking Panel (Gemini) */}
        {(() => {
          const hasContent = !!(modelThinkingDisplay || modelThinkingFull)
          const forceShow = thinkingChunkCountRef.current > 0 // Force show if we have chunks
          const shouldShow = (hasContent || forceShow) && !isGenerationComplete
          return shouldShow
        })() && (
          <div className="mt-2">
            <button
              type="button"
              className="flex items-center justify-between w-full text-left text-xs uppercase tracking-wide text-slate-300 bg-slate-800/40 hover:bg-slate-800/60 border border-slate-600/40 px-3 py-2 rounded"
              onClick={() => setIsModelThinkingOpen(!isModelThinkingOpen)}
              aria-expanded={isModelThinkingOpen}
            >
              <span>Model thoughts ({thinkingChunkCountRef.current} chunks)</span>
              {isModelThinkingOpen ? <ChevronDown className="h-3 w-3" /> : <ChevronRight className="h-3 w-3" />}
            </button>
            {isModelThinkingOpen && (
              <div className="mt-2 p-3 rounded-lg border border-slate-500/20 bg-slate-800/20 text-white text-sm whitespace-pre-wrap leading-relaxed max-h-48 overflow-auto italic">
                {modelThinkingDisplay || modelThinkingFull}
              </div>
            )}
          </div>
        )}

        <div ref={messagesEndRef} />
      </div>

      {/* Input */}
      <div className="p-4 border-t border-white/10">
        <form onSubmit={handleSendMessage} className="space-y-3">
          <Textarea
            value={inputMessage}
            onChange={(e) => setInputMessage(e.target.value)}
            placeholder={projectName ? `describe what you want to add or modify in ${projectName}...` : "describe what you want to add or modify..."}
            className="min-h-[80px] bg-slate-700/50 border-slate-600 text-white placeholder:text-slate-400 resize-none"
          />
          <Button
            type="submit"
            disabled={!inputMessage.trim() || isGenerating || !projectId}
            className="w-full bg-gradient-to-r from-purple-500 to-blue-500 hover:from-purple-600 hover:to-blue-600 disabled:opacity-50"
          >
            {isGenerating ? (
              <>
                <Loader2 className="h-4 w-4 mr-2 animate-spin" />
                generating...
              </>
            ) : !projectId ? (
              "setting up project..."
            ) : (
              <>
                <Send className="h-4 w-4 mr-2" />
                send
              </>
            )}
          </Button>
        </form>
      </div>

      {/* URL Prompt Modal */}
      {showUrlPrompt && urlPromptData && (
        <ModalUrlPrompt
          data={urlPromptData.data}
          originalPrompt={urlPromptData.originalPrompt}
          onUrlSubmit={handleUrlSubmit}
          onCancel={handleUrlCancel}
          onCodeGenerated={onCodeGenerated}
          projectId={projectId}
          hasGeneratedCode={hasGeneratedCode}
          onGenerationEnd={onGenerationEnd}
        />
      )}

      {/* API Prompt Modal */}
      {showApiPrompt && apiPromptData && (
        <ModalApiPrompt
          data={apiPromptData.data}
          originalPrompt={apiPromptData.originalPrompt}
          onApiSubmit={handleApiSubmit}
          onCancel={handleApiCancel}
        />
      )}
    </div>
  )
}
